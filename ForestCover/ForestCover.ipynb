{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Forect Cover Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are trying to predict the forest cover type of a land using features such a elevation, soil type, slope, etc. This is a supervised learning problem and we have a dataset readily available with training examples and the true target value. Furthermore, this is a classification type problem as for each example we are trying to classify it into one of the 7 pre-defined forest cover type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses pyspark 3.0 API for distributed machine learning on big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "import pandas as pd\n",
    "from pprint import PrettyPrinter\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using spark in the standalone mode with single node cluster. For other configurations, see spark's documentation [here](https://spark.apache.org/docs/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SparkSession object\n",
    "spark = SparkSession.builder.appName('ForestCover').master(\"spark://DESKTOP-H1P4CCM.localdomain:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "base = os.getcwd()\n",
    "filepath = \"datasets/covtype/covtype.data\"\n",
    "colNames = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\\\n",
    "            \"Horizontal_Distance_To_Roadways\", \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\\\n",
    "            \"Horizontal_Distance_To_Fire_Points\"] + [\"Wilderness_Area_\"+str(i) for i in range(4)]\\\n",
    "+ [\"Soil_Type_\"+str(i) for i in range(40)] + ['Cover_Type']\n",
    "labelCol = \"Cover_Type\"\n",
    "pp = PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this excercise, I am using Covtype dataset avaliable [online](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/) as compressed zip file and the associted info file. The dataset contains records of land patches in Colarado, USA and their associated forest cover type. Each record contains 54 properties that collectively describe a land patch. Of the 54 properties, 10 are numerical and 2 categorical which are one-hot encoded and spread out into 44 propeties (4 wilderness and 40 soil type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: integer (nullable = true)\n",
      " |-- _c25: integer (nullable = true)\n",
      " |-- _c26: integer (nullable = true)\n",
      " |-- _c27: integer (nullable = true)\n",
      " |-- _c28: integer (nullable = true)\n",
      " |-- _c29: integer (nullable = true)\n",
      " |-- _c30: integer (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: integer (nullable = true)\n",
      " |-- _c34: integer (nullable = true)\n",
      " |-- _c35: integer (nullable = true)\n",
      " |-- _c36: integer (nullable = true)\n",
      " |-- _c37: integer (nullable = true)\n",
      " |-- _c38: integer (nullable = true)\n",
      " |-- _c39: integer (nullable = true)\n",
      " |-- _c40: integer (nullable = true)\n",
      " |-- _c41: integer (nullable = true)\n",
      " |-- _c42: integer (nullable = true)\n",
      " |-- _c43: integer (nullable = true)\n",
      " |-- _c44: integer (nullable = true)\n",
      " |-- _c45: integer (nullable = true)\n",
      " |-- _c46: integer (nullable = true)\n",
      " |-- _c47: integer (nullable = true)\n",
      " |-- _c48: integer (nullable = true)\n",
      " |-- _c49: integer (nullable = true)\n",
      " |-- _c50: integer (nullable = true)\n",
      " |-- _c51: integer (nullable = true)\n",
      " |-- _c52: integer (nullable = true)\n",
      " |-- _c53: integer (nullable = true)\n",
      " |-- _c54: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read csv file\n",
    "covtype = spark.read.option('header', False).option('inferSchema', True).csv(os.path.join(base, filepath))\n",
    "covtype.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Wilderness_Area_0: integer (nullable = true)\n",
      " |-- Wilderness_Area_1: integer (nullable = true)\n",
      " |-- Wilderness_Area_2: integer (nullable = true)\n",
      " |-- Wilderness_Area_3: integer (nullable = true)\n",
      " |-- Soil_Type_0: integer (nullable = true)\n",
      " |-- Soil_Type_1: integer (nullable = true)\n",
      " |-- Soil_Type_2: integer (nullable = true)\n",
      " |-- Soil_Type_3: integer (nullable = true)\n",
      " |-- Soil_Type_4: integer (nullable = true)\n",
      " |-- Soil_Type_5: integer (nullable = true)\n",
      " |-- Soil_Type_6: integer (nullable = true)\n",
      " |-- Soil_Type_7: integer (nullable = true)\n",
      " |-- Soil_Type_8: integer (nullable = true)\n",
      " |-- Soil_Type_9: integer (nullable = true)\n",
      " |-- Soil_Type_10: integer (nullable = true)\n",
      " |-- Soil_Type_11: integer (nullable = true)\n",
      " |-- Soil_Type_12: integer (nullable = true)\n",
      " |-- Soil_Type_13: integer (nullable = true)\n",
      " |-- Soil_Type_14: integer (nullable = true)\n",
      " |-- Soil_Type_15: integer (nullable = true)\n",
      " |-- Soil_Type_16: integer (nullable = true)\n",
      " |-- Soil_Type_17: integer (nullable = true)\n",
      " |-- Soil_Type_18: integer (nullable = true)\n",
      " |-- Soil_Type_19: integer (nullable = true)\n",
      " |-- Soil_Type_20: integer (nullable = true)\n",
      " |-- Soil_Type_21: integer (nullable = true)\n",
      " |-- Soil_Type_22: integer (nullable = true)\n",
      " |-- Soil_Type_23: integer (nullable = true)\n",
      " |-- Soil_Type_24: integer (nullable = true)\n",
      " |-- Soil_Type_25: integer (nullable = true)\n",
      " |-- Soil_Type_26: integer (nullable = true)\n",
      " |-- Soil_Type_27: integer (nullable = true)\n",
      " |-- Soil_Type_28: integer (nullable = true)\n",
      " |-- Soil_Type_29: integer (nullable = true)\n",
      " |-- Soil_Type_30: integer (nullable = true)\n",
      " |-- Soil_Type_31: integer (nullable = true)\n",
      " |-- Soil_Type_32: integer (nullable = true)\n",
      " |-- Soil_Type_33: integer (nullable = true)\n",
      " |-- Soil_Type_34: integer (nullable = true)\n",
      " |-- Soil_Type_35: integer (nullable = true)\n",
      " |-- Soil_Type_36: integer (nullable = true)\n",
      " |-- Soil_Type_37: integer (nullable = true)\n",
      " |-- Soil_Type_38: integer (nullable = true)\n",
      " |-- Soil_Type_39: integer (nullable = true)\n",
      " |-- Cover_Type: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming the columns\n",
    "data = covtype.toDF(*colNames).withColumn(labelCol, f.col(labelCol).cast(\"double\"))\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------\n",
      " Elevation                          | 2596 \n",
      " Aspect                             | 51   \n",
      " Slope                              | 3    \n",
      " Horizontal_Distance_To_Hydrology   | 258  \n",
      " Vertical_Distance_To_Hydrology     | 0    \n",
      " Horizontal_Distance_To_Roadways    | 510  \n",
      " Hillshade_9am                      | 221  \n",
      " Hillshade_Noon                     | 232  \n",
      " Hillshade_3pm                      | 148  \n",
      " Horizontal_Distance_To_Fire_Points | 6279 \n",
      " Wilderness_Area_0                  | 1    \n",
      " Wilderness_Area_1                  | 0    \n",
      " Wilderness_Area_2                  | 0    \n",
      " Wilderness_Area_3                  | 0    \n",
      " Soil_Type_0                        | 0    \n",
      " Soil_Type_1                        | 0    \n",
      " Soil_Type_2                        | 0    \n",
      " Soil_Type_3                        | 0    \n",
      " Soil_Type_4                        | 0    \n",
      " Soil_Type_5                        | 0    \n",
      " Soil_Type_6                        | 0    \n",
      " Soil_Type_7                        | 0    \n",
      " Soil_Type_8                        | 0    \n",
      " Soil_Type_9                        | 0    \n",
      " Soil_Type_10                       | 0    \n",
      " Soil_Type_11                       | 0    \n",
      " Soil_Type_12                       | 0    \n",
      " Soil_Type_13                       | 0    \n",
      " Soil_Type_14                       | 0    \n",
      " Soil_Type_15                       | 0    \n",
      " Soil_Type_16                       | 0    \n",
      " Soil_Type_17                       | 0    \n",
      " Soil_Type_18                       | 0    \n",
      " Soil_Type_19                       | 0    \n",
      " Soil_Type_20                       | 0    \n",
      " Soil_Type_21                       | 0    \n",
      " Soil_Type_22                       | 0    \n",
      " Soil_Type_23                       | 0    \n",
      " Soil_Type_24                       | 0    \n",
      " Soil_Type_25                       | 0    \n",
      " Soil_Type_26                       | 0    \n",
      " Soil_Type_27                       | 0    \n",
      " Soil_Type_28                       | 1    \n",
      " Soil_Type_29                       | 0    \n",
      " Soil_Type_30                       | 0    \n",
      " Soil_Type_31                       | 0    \n",
      " Soil_Type_32                       | 0    \n",
      " Soil_Type_33                       | 0    \n",
      " Soil_Type_34                       | 0    \n",
      " Soil_Type_35                       | 0    \n",
      " Soil_Type_36                       | 0    \n",
      " Soil_Type_37                       | 0    \n",
      " Soil_Type_38                       | 0    \n",
      " Soil_Type_39                       | 0    \n",
      " Cover_Type                         | 5.0  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "data.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable for this problem is a categorical variable with 7 categories each of which represents a unique forest coverage type. As we can see from the record counts for each of the category of target variable, the dataset is highly imabalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|Cover_Type| count|\n",
      "+----------+------+\n",
      "|       1.0|211840|\n",
      "|       2.0|283301|\n",
      "|       3.0| 35754|\n",
      "|       4.0|  2747|\n",
      "|       5.0|  9493|\n",
      "|       6.0| 17367|\n",
      "|       7.0| 20510|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Target\n",
    "data.groupBy(labelCol).count().orderBy(labelCol).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using Spark's MLlib library to perform multi-class classification on this dataset. The data is first split into train and test sets. I will then calculate the baseline accuracy using random guessing based on proportions of records of each class in training and test datasets. After that, I will train a simple DecisionTree learner on the training set without any parameter tuning and compare its performance with the baseline model. I will then proceed to to use a grid search model, from spark's MLlib, to tune the decision tree, using k-fold cross-validation, and improve its performance. I will also undo the one-hot encoding of some categorical variables to create a smaller, efficient dataset. In the end, I will use a more powerful RandomForest model to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test sets\n",
    "train, test = data.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProbabilities(data, label):\n",
    "    \"\"\"\n",
    "    The function takes a spark DataFrame as input and returns the probability of selecting different values in label column.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data : pyspark.sql.DataFrame\n",
    "        PySpark DataFrame containing the dataset\n",
    "    label: str\n",
    "        Name of the target variable in dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tuples where the first element is a unique category in target variable and the second element specifies the\n",
    "        proportion of records in the dataset that belong to that category.\n",
    "    \"\"\"\n",
    "    total = data.count()\n",
    "    return data.select(label).groupBy(label).count().orderBy(label).rdd.map(lambda x: (x[0], x[1]/total)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline accuracy is: 37.704\n"
     ]
    }
   ],
   "source": [
    "# Proportion of each category in training data\n",
    "trainProbabilities = calculateProbabilities(train, labelCol)\n",
    "testProbabilities = calculateProbabilities(test, labelCol)\n",
    "baseline_accuracy = sum([x[1]*y[1] for x, y in zip(trainProbabilities, testProbabilities)])\n",
    "print(\"The baseline accuracy is:\", round(baseline_accuracy*100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage Collection\n",
    "del trainProbabilities, testProbabilities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example there's no need to define transformations since the dataset is already clean and in a suitable format. Since all the classes in MLlib module require that the feature be a single vector, I will just use the VectorAssembler class to create feature vector for each record by packing all the properties in a single vector. Any other required features should be defined in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " Elevation                          | 1863                                                                                                \n",
      " Aspect                             | 37                                                                                                  \n",
      " Slope                              | 17                                                                                                  \n",
      " Horizontal_Distance_To_Hydrology   | 120                                                                                                 \n",
      " Vertical_Distance_To_Hydrology     | 18                                                                                                  \n",
      " Horizontal_Distance_To_Roadways    | 90                                                                                                  \n",
      " Hillshade_9am                      | 217                                                                                                 \n",
      " Hillshade_Noon                     | 202                                                                                                 \n",
      " Hillshade_3pm                      | 115                                                                                                 \n",
      " Horizontal_Distance_To_Fire_Points | 769                                                                                                 \n",
      " Wilderness_Area_0                  | 0                                                                                                   \n",
      " Wilderness_Area_1                  | 0                                                                                                   \n",
      " Wilderness_Area_2                  | 0                                                                                                   \n",
      " Wilderness_Area_3                  | 1                                                                                                   \n",
      " Soil_Type_0                        | 0                                                                                                   \n",
      " Soil_Type_1                        | 1                                                                                                   \n",
      " Soil_Type_2                        | 0                                                                                                   \n",
      " Soil_Type_3                        | 0                                                                                                   \n",
      " Soil_Type_4                        | 0                                                                                                   \n",
      " Soil_Type_5                        | 0                                                                                                   \n",
      " Soil_Type_6                        | 0                                                                                                   \n",
      " Soil_Type_7                        | 0                                                                                                   \n",
      " Soil_Type_8                        | 0                                                                                                   \n",
      " Soil_Type_9                        | 0                                                                                                   \n",
      " Soil_Type_10                       | 0                                                                                                   \n",
      " Soil_Type_11                       | 0                                                                                                   \n",
      " Soil_Type_12                       | 0                                                                                                   \n",
      " Soil_Type_13                       | 0                                                                                                   \n",
      " Soil_Type_14                       | 0                                                                                                   \n",
      " Soil_Type_15                       | 0                                                                                                   \n",
      " Soil_Type_16                       | 0                                                                                                   \n",
      " Soil_Type_17                       | 0                                                                                                   \n",
      " Soil_Type_18                       | 0                                                                                                   \n",
      " Soil_Type_19                       | 0                                                                                                   \n",
      " Soil_Type_20                       | 0                                                                                                   \n",
      " Soil_Type_21                       | 0                                                                                                   \n",
      " Soil_Type_22                       | 0                                                                                                   \n",
      " Soil_Type_23                       | 0                                                                                                   \n",
      " Soil_Type_24                       | 0                                                                                                   \n",
      " Soil_Type_25                       | 0                                                                                                   \n",
      " Soil_Type_26                       | 0                                                                                                   \n",
      " Soil_Type_27                       | 0                                                                                                   \n",
      " Soil_Type_28                       | 0                                                                                                   \n",
      " Soil_Type_29                       | 0                                                                                                   \n",
      " Soil_Type_30                       | 0                                                                                                   \n",
      " Soil_Type_31                       | 0                                                                                                   \n",
      " Soil_Type_32                       | 0                                                                                                   \n",
      " Soil_Type_33                       | 0                                                                                                   \n",
      " Soil_Type_34                       | 0                                                                                                   \n",
      " Soil_Type_35                       | 0                                                                                                   \n",
      " Soil_Type_36                       | 0                                                                                                   \n",
      " Soil_Type_37                       | 0                                                                                                   \n",
      " Soil_Type_38                       | 0                                                                                                   \n",
      " Soil_Type_39                       | 0                                                                                                   \n",
      " Cover_Type                         | 6.0                                                                                                 \n",
      " featureVector                      | (54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0]) \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using VectorAssembler class to generate feature vector\n",
    "feature_cols = [col for col in data.columns if col!=labelCol]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"featureVector\")\n",
    "trainData = assembler.transform(train)\n",
    "trainData.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a simple model that uses human-like thought process in making decisions. It creates a tree like structure in which the internal node specifies the condition that determines the path and the leave nodes determine the label that a record is assigned, if it satisfies all the conditions on the path to that leaf node. The decision tree is constructed in such a way that after each split from an internal node, the training data gets \"purer\", that is, more homogeneous (majority of the records belong to same class). We use entropy or gini-index measures that help us determine \"node purity\" to identify the perfect split at each step.\n",
    "\n",
    "**Advantages:**\n",
    "* Easy to build\n",
    "* Easy to interpret\n",
    "* Robust to outliers\n",
    "\n",
    "**Disadvantages:**:\n",
    "* Weak learner\n",
    "* Prone to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DecisionTreeClassificationModel: uid=DecisionTreeClassifier_9a8cbdd3864a, '\n",
      " 'depth=5, numNodes=49, numClasses=8, numFeatures=54\\n'\n",
      " '  If (feature 0 <= 3051.5)\\n'\n",
      " '   If (feature 0 <= 2572.5)\\n'\n",
      " '    If (feature 10 <= 0.5)\\n'\n",
      " '     If (feature 0 <= 2471.5)\\n'\n",
      " '      If (feature 3 <= 15.0)\\n'\n",
      " '       Predict: 4.0\\n'\n",
      " '      Else (feature 3 > 15.0)\\n'\n",
      " '       Predict: 3.0\\n'\n",
      " '     Else (feature 0 > 2471.5)\\n'\n",
      " '      If (feature 17 <= 0.5)\\n'\n",
      " '       Predict: 2.0\\n'\n",
      " '      Else (feature 17 > 0.5)\\n'\n",
      " '       Predict: 3.0\\n'\n",
      " '    Else (feature 10 > 0.5)\\n'\n",
      " '     If (feature 9 <= 5080.0)\\n'\n",
      " '      Predict: 2.0\\n'\n",
      " '     Else (feature 9 > 5080.0)\\n'\n",
      " '      If (feature 31 <= 0.5)\\n'\n",
      " '       Predict: 2.0\\n'\n",
      " '      Else (feature 31 > 0.5)\\n'\n",
      " '       Predict: 5.0\\n'\n",
      " '   Else (feature 0 > 2572.5)\\n'\n",
      " '    If (feature 0 <= 2961.5)\\n'\n",
      " '     If (feature 15 <= 0.5)\\n'\n",
      " '      Predict: 2.0\\n'\n",
      " '     Else (feature 15 > 0.5)\\n'\n",
      " '      Predict: 3.0\\n'\n",
      " '    Else (feature 0 > 2961.5)\\n'\n",
      " '     If (feature 3 <= 211.0)\\n'\n",
      " '      If (feature 36 <= 0.5)\\n'\n",
      " '       Predict: 2.0\\n'\n",
      " '      Else (feature 36 > 0.5)\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '     Else (feature 3 > 211.0)\\n'\n",
      " '      Predict: 2.0\\n'\n",
      " '  Else (feature 0 > 3051.5)\\n'\n",
      " '   If (feature 0 <= 3336.5)\\n'\n",
      " '    If (feature 7 <= 239.5)\\n'\n",
      " '     If (feature 0 <= 3154.5)\\n'\n",
      " '      If (feature 45 <= 0.5)\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '      Else (feature 45 > 0.5)\\n'\n",
      " '       Predict: 2.0\\n'\n",
      " '     Else (feature 0 > 3154.5)\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '    Else (feature 7 > 239.5)\\n'\n",
      " '     If (feature 3 <= 316.0)\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '     Else (feature 3 > 316.0)\\n'\n",
      " '      If (feature 0 <= 3218.5)\\n'\n",
      " '       Predict: 2.0\\n'\n",
      " '      Else (feature 0 > 3218.5)\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '   Else (feature 0 > 3336.5)\\n'\n",
      " '    If (feature 12 <= 0.5)\\n'\n",
      " '     If (feature 4 <= 51.5)\\n'\n",
      " '      If (feature 7 <= 237.5)\\n'\n",
      " '       Predict: 7.0\\n'\n",
      " '      Else (feature 7 > 237.5)\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '     Else (feature 4 > 51.5)\\n'\n",
      " '      Predict: 1.0\\n'\n",
      " '    Else (feature 12 > 0.5)\\n'\n",
      " '     If (feature 45 <= 0.5)\\n'\n",
      " '      If (feature 9 <= 457.5)\\n'\n",
      " '       Predict: 1.0\\n'\n",
      " '      Else (feature 9 > 457.5)\\n'\n",
      " '       Predict: 7.0\\n'\n",
      " '     Else (feature 45 > 0.5)\\n'\n",
      " '      If (feature 5 <= 913.0)\\n'\n",
      " '       Predict: 7.0\\n'\n",
      " '      Else (feature 5 > 913.0)\\n'\n",
      " '       Predict: 1.0\\n')\n"
     ]
    }
   ],
   "source": [
    "# DecisionTree Classification model\n",
    "classifier = DecisionTreeClassifier(featuresCol='featureVector', labelCol=labelCol)\n",
    "model = classifier.fit(trainData)\n",
    "pp.pprint(model.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8130894670615963, 'Elevation'),\n",
      " (0.038054619229894907, 'Horizontal_Distance_To_Hydrology'),\n",
      " (0.036896773869442706, 'Wilderness_Area_0'),\n",
      " (0.02660125506316331, 'Soil_Type_31'),\n",
      " (0.026048506327293056, 'Hillshade_Noon'),\n",
      " (0.02225856699970976, 'Soil_Type_1'),\n",
      " (0.01095773511562616, 'Wilderness_Area_2'),\n",
      " (0.010646403214294222, 'Soil_Type_3'),\n",
      " (0.006123390468124926, 'Soil_Type_22'),\n",
      " (0.004076612931206928, 'Horizontal_Distance_To_Fire_Points'),\n",
      " (0.003342119314331825, 'Vertical_Distance_To_Hydrology'),\n",
      " (0.001479541173056448, 'Horizontal_Distance_To_Roadways'),\n",
      " (0.000425009232259348, 'Soil_Type_17'),\n",
      " (0.0, 'Aspect'),\n",
      " (0.0, 'Slope'),\n",
      " (0.0, 'Hillshade_9am'),\n",
      " (0.0, 'Hillshade_3pm'),\n",
      " (0.0, 'Wilderness_Area_1'),\n",
      " (0.0, 'Wilderness_Area_3'),\n",
      " (0.0, 'Soil_Type_0'),\n",
      " (0.0, 'Soil_Type_2'),\n",
      " (0.0, 'Soil_Type_4'),\n",
      " (0.0, 'Soil_Type_5'),\n",
      " (0.0, 'Soil_Type_6'),\n",
      " (0.0, 'Soil_Type_7'),\n",
      " (0.0, 'Soil_Type_8'),\n",
      " (0.0, 'Soil_Type_9'),\n",
      " (0.0, 'Soil_Type_10'),\n",
      " (0.0, 'Soil_Type_11'),\n",
      " (0.0, 'Soil_Type_12'),\n",
      " (0.0, 'Soil_Type_13'),\n",
      " (0.0, 'Soil_Type_14'),\n",
      " (0.0, 'Soil_Type_15'),\n",
      " (0.0, 'Soil_Type_16'),\n",
      " (0.0, 'Soil_Type_18'),\n",
      " (0.0, 'Soil_Type_19'),\n",
      " (0.0, 'Soil_Type_20'),\n",
      " (0.0, 'Soil_Type_21'),\n",
      " (0.0, 'Soil_Type_23'),\n",
      " (0.0, 'Soil_Type_24'),\n",
      " (0.0, 'Soil_Type_25'),\n",
      " (0.0, 'Soil_Type_26'),\n",
      " (0.0, 'Soil_Type_27'),\n",
      " (0.0, 'Soil_Type_28'),\n",
      " (0.0, 'Soil_Type_29'),\n",
      " (0.0, 'Soil_Type_30'),\n",
      " (0.0, 'Soil_Type_32'),\n",
      " (0.0, 'Soil_Type_33'),\n",
      " (0.0, 'Soil_Type_34'),\n",
      " (0.0, 'Soil_Type_35'),\n",
      " (0.0, 'Soil_Type_36'),\n",
      " (0.0, 'Soil_Type_37'),\n",
      " (0.0, 'Soil_Type_38'),\n",
      " (0.0, 'Soil_Type_39')]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "pp.pprint(sorted(list(zip(model.featureImportances.toArray(), feature_cols)), key = lambda x: x[0], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------\n",
      " Cover_Type    | 6.0                                                                                              \n",
      " rawPrediction | [0.0,0.0,1338.0,19433.0,1488.0,0.0,9008.0,0.0]                                                   \n",
      " probability   | [0.0,0.0,0.04279272075990661,0.6215178942655195,0.047590110979627086,0.0,0.2880992739949467,0.0] \n",
      " prediction    | 3.0                                                                                              \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction on training data\n",
    "predictions = model.transform(trainData)\n",
    "predictions.select(labelCol, \"rawPrediction\", \"probability\", \"prediction\").show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7052293365801259\n",
      "f1 score: 0.687221249734038\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=labelCol)\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", evaluator.setMetricName('accuracy').evaluate(predictions))\n",
    "# F-1 score\n",
    "print(\"f1 score:\", evaluator.setMetricName('f1').evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a NxN matrix, where N is the number of distinct categories of target variable, and used to test the accuracy of a classifier. The (i,j)th element specifies the number of records that actually belong to class i that were determined to belong to class j by the classifier. The sum of elements on the main diagonal is the number of records correctly classified by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix using old MLlib API\n",
    "# predictionsRDD = predictions.select(\"prediction\", \"Cover_Type\").rdd\n",
    "# multiClassMetrics = MulticlassMetrics(predictionsRDD)\n",
    "# confusionMatrix = pd.DataFrame(multiClassMetrics.confusionMatrix().toArray())\n",
    "# confusionMatrix.index = range(1,8)\n",
    "# confusionMatrix.columns = range(1,8)\n",
    "# confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+-----+----+---+---+----+\n",
      "|Cover_Type|     1|     2|    3|   4|  5|  6|   7|\n",
      "+----------+------+------+-----+----+---+---+----+\n",
      "|       1.0|130587| 56517|    0|   2|  0|  0|3471|\n",
      "|       2.0| 48306|204389| 1886| 131|107|  0| 400|\n",
      "|       3.0|     0|  7837|23667| 674|  0|  0|   0|\n",
      "|       4.0|     0|    14| 1488|1011|  0|  0|   0|\n",
      "|       5.0|     0|  8338|  112|   0|119|  0|   0|\n",
      "|       6.0|     0|  5552| 9462| 647|  0|  0|   0|\n",
      "|       7.0|  9200|    75|    0|   0|  0|  0|9191|\n",
      "+----------+------+------+-----+----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix using pivot\n",
    "confusionMatrix = predictions.groupBy(labelCol).pivot(\"prediction\", range(1, 8)).count().na.fill(0.0).orderBy(labelCol)\n",
    "confusionMatrix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage Collection\n",
    "del assembler, trainData, classifier, model, predictions, evaluator, confusionMatrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am going to define a pipeline that combines the transformation and modelling steps to make the process more robust, reproducible and easier to manage. I will also use a grid search method, spark's MLlib implementation, for optimizing the hyperparameters of the earlier model. The grid search will use cross-validation to determine the best model with optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler to create feaure vector\n",
    "feature_cols = [col for col in data.columns if col!=labelCol]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"featureVector\")\n",
    "# RandomForest classifier\n",
    "classifier = RandomForestClassifier(featuresCol='featureVector', labelCol=labelCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "paramGridSearch = ParamGridBuilder().\\\n",
    "                    addGrid(classifier.impurity, ['gini', 'entropy']).\\\n",
    "                    addGrid(classifier.maxDepth, [1, 20]).\\\n",
    "                    addGrid(classifier.maxBins, [40, 300]).\\\n",
    "                    addGrid(classifier.minInfoGain, [0, 0.05]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=labelCol, metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for grid search\n",
    "crossValidation = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGridSearch, evaluator=evaluator, trainRatio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best model from cross-validation\n",
    "# model = crossValidation.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garbage Collection\n",
    "del train, test, assembler, classifier, pipeline, paramGridSearch, evaluator, crossValidation, model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, two properties, namely, wilderness area and soil type, are one-hot encoded in the original data. While one-hot encoding is a popular choice to represent categorical variables to transform the data suitable for machine learning algorithms, it does take a lot of space since the same data is spread out in multiple columns. In addition to that, there are many machine learning algorithms that can handle categorical features, like Decision Tree, and hence I am going to remove the one-hot encoding from the dataset to decrease its size. For this purpose, I am going to use VectorAssemebler to first collect relevant columns into single vector and then pandas UDF to define a function that removes the one-hot encoding. After the one-hot encoding is removed, we obtain columns that numeric values to indicate categories. It is necessary to add metadata information about these columns to indicate that they are actually categorical. For this purpose an instance of VectorIndexer will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@f.pandas_udf(t.IntegerType())\n",
    "def removeOHE(se):\n",
    "    \"\"\"\n",
    "    Takes a pandas series such that each element in the series is an iterable and returns the index of largest element in iterable.\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    se: pandas.Series\n",
    "        Series containing iterables\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series containing integers\n",
    "    \"\"\"\n",
    "    return se.map(np.argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping together columns that are one-hot encoding of the same feature\n",
    "wildernessCols = [\"Wilderness_Area_\"+str(i) for i in range(4)]\n",
    "soilCols = [\"Soil_Type_\"+str(i) for i in range(40)]\n",
    "wildernessAssembler = VectorAssembler(inputCols=wildernessCols, outputCol=\"Wilderness_Area\")\n",
    "soilAssembler = VectorAssembler(inputCols=soilCols, outputCol=\"Soil_Type\")\n",
    "pipeRemoveOHE = Pipeline(stages=[wildernessAssembler, soilAssembler])\n",
    "pipeRemoveOHEModel = pipeRemoveOHE.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: integer (nullable = true)\n",
      " |-- Aspect: integer (nullable = true)\n",
      " |-- Slope: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n",
      " |-- Hillshade_9am: integer (nullable = true)\n",
      " |-- Hillshade_Noon: integer (nullable = true)\n",
      " |-- Hillshade_3pm: integer (nullable = true)\n",
      " |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n",
      " |-- Cover_Type: double (nullable = true)\n",
      " |-- Wilderness_Area: integer (nullable = true)\n",
      " |-- Soil_Type: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################################################################################\n",
    "# Note: Create a custom transformer that applies the function on a column\n",
    "##############################################################################################################\n",
    "\n",
    "# Transforming the dataset to remove one-hot encoding\n",
    "data = pipeRemoveOHEModel.transform(data).drop(*(wildernessCols+soilCols)).withColumn(\"Wilderness_Area\", removeOHE(\"Wilderness_Area\")).withColumn(\"Soil_Type\", removeOHE(\"Soil_Type\"))\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into training and test sets\n",
    "train, test = data.randomSplit([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler to create feature vector for each record\n",
    "feature_cols = [col for col in train.columns if col!=labelCol]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"featureVector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorIndexer to mark categorical variables\n",
    "indexer = new VectorIndexer(maxCategories=40, inputCol=\"featureVector\", outputCol=\"modifiedFeatureVector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Decision Forest or Random Forest for short are powerful ensemble learning methods for regression, classification and other tasks. Random Forests are built by taking an ensemble of multiple Decision Trees built independently using only a random subset of the dataset, called bagging, with each record in this subset having a random subset of all the features. The randomness and independence in construction of each Decision Tree ensures that the resulting ensemble regresses towards the correct answer, hence more powerful than any of the Decision Trees individually. Bagging also helps reduce overfitting since each tree only sees a portion of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest Classifier\n",
    "classifier = RandomForestClassifier(featuresCol='modifiedFeatureVector', labelCol='Cover_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "del spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
